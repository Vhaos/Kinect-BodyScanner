<!DOCTYPE html>
<html>

<head>
  <title>Kinect Bodyscanner</title>
  <!-- Load platform support before anything else -->
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <link rel="icon" type="type/png" href="img/Misc/k-icon.png"/>
  <script src="bower_components/webcomponentsjs/webcomponents.js"></script>

  <link rel="stylesheet" href="main.css">
  <link rel="import" href="bower_components/font-roboto/roboto.html">
  <link rel="import" href="bower_components/core-header-panel/core-header-panel.html">
  <link rel="import" href="bower_components/core-toolbar/core-toolbar.html">
  <link rel="import" href="elements/card-content.html">
  <link rel="import" href="elements/card-banner.html">
  <link rel="import" href="elements/common-links.html">

</head>


<body unresolved fullbleed layout vertical>

  <core-header-panel layout vertical>

    <a name="top"></a>
    <core-toolbar>
      <h2><a href="#top">Kinect Bodyscanner</a></h2>
    </core-toolbar>


    <common-links select='5'></common-links>


    <card-banner solid=true>
      <h1>Prototypes</h1>
    </card-banner>

    <card-content>
      <h2>Sensor Modification</h2>
      <p>To reduce cost, we decided to use the Kinect 2.0 that is available with Xbox One. This meant we had to modify the wiring to make it work with Windows. Originally the Kinect came with a proprietary connector that plugs into Xbox. We had to split the connector into Power and USB 3.0 connectors.</p>
      <img src="img/Prototyping/Connectors.png">
    </card-content>

    <card-content>
      <p>Here are some images of the modification. A step-by-step process can be found <a style="text-decoration: underline;" href=""><b>here</b></a>.</p>
    </card-content>

    <div layout horizontal around-justified>
      <img style="border-radius: 10px;" width="300px" height="200px" src="img/Tutorial/12_data_cables2.jpg">
      <img style="border-radius: 10px;" width="300px" height="200px" src="img/Tutorial/27_solder1.jpg">
    </div>
    <br>
    <br>
    <card-content>
      <h2>Choices</h2>
      <p><b>Operating System</b>
        <br>We are using the Kinect sensor for this project, naturally leading to our use of Windows, as Microsoft provides drivers and documentation exclusively for their platform. We considered using Linux as open source drivers do exist online; however, advanced configuartion, as well as mainstream adoptance of Windows in businesses and homes makes it the better choice in that regard.</p>
      <p><b>Programming Languages</b>
        <br> We were in dilemma as to which language to use for this project. We considered two programming languages: C++ and C#.</p>
    </card-content>
    <div layout horizontal around-justified>
      <div>
        <b>C++</b>
        <ul>
          <b>Pros</b>
          <li>Plenty of 3rd party libraries, especially for 3D point cloud analysis</li>
          <b>Cons</b>
          <li>Less direct support for Kinect</li>
        </ul>
      </div>
      <div>
        <b>C#</b>
        <ul>
          <b>Pros</b>
          <li>Official support from Microsoft</li>
          <li>Official APIs available</li>
          <li>Most tutorials on Kinect in C#</li>
          <b>Cons</b>
          <li>Not many third party libraries</li>
        </ul>
      </div>
    </div>
    <card-content>
      <p>In the end, we decided to use C# as our main language when programming the sensor. However, this does not prevent us from using third party libraries written in C++ language using a wrapper in C#.</p>

      <p><b>Toolchain</b><br>Following the choices made for the operating system and programming language, it was natural to use Microsoft Visual Studio as our IDE, as Microsoft has official tutorials and documentation based around it. We used Git for Version control, as all the members in our groups were familiar with it and we used GitHub to host our Git repo remotely.</p>
    </card-content>

    <card-content>
      <h2>System Prototype</h2>
      <p>For our prototype, we had to utilise existing software and design our body scanner accordingly. We spoke to Dr. Tony Ruto who is an expert in this field and is familiar with the existing software. He asked us to provide him with a 3D point-cloud of the body for him to test with the existing software with the aim of extracting body measurements. As a result, term 1 has primarily been focused on mdofiying the Kinects, extracting the point cloud from the Kinect 2.0 sensor and designing the user interface.</p>
    </card-content>
    <card-banner>
      <h1>System Activity Diagram</h1>
    </card-banner>
    <div layout horizontal center-justified>
      <img style="border: 1px solid black;" max-height="500px" src="img/arch_diagram/SequenceDiagram.jpg">
    </div>
    <card-banner>
      <h1>Windows UML</h1>
    </card-banner>
    <div layout horizontal center-justified>
      <a href="img/arch_diagram/WindowsUML.jpg"><img style="padding: 5px 5px; border: 1px solid black; max-height: 300px;"  src="img/arch_diagram/WindowsUML.jpg"></a>
    </div>
    <card-banner>
      <h1>Android UML</h1>
    </card-banner>
    <div layout horizontal center-justified>
      <a href="img/arch_diagram/AndroidUML.jpg"><img style="padding: 5px 5px; border: 1px solid black; max-height: 300px;"  src="img/arch_diagram/AndroidUML.jpg"></a>
    </div>

    <card-content>
      <p><b>Computing the Point Cloud</b><br><br>
        In order to extract the point cloud:
        <ul>
        <li>We needed to get the depth data for every pixel, using the depth frame source reader</li>
          <li>Use the Coordinate Mapper class to map the depth frame data to camera space point objects, which store 3D coordinate values for every pixel</li>
          <li>Use BodyIndexFrame to exclude pixels that do not comprise the human body</li>
          <li>Write the 3d coordinates from the remaining camera space points to a file, hence creating a pointcloud</li>
      </ul>
      </p>
      <p><b>Extracting the Measurements</b><br>
      To extract the measurements, we need to train a classifier algorithm to recognize which points correspond to which parts of the body. However, this has already been done by UCL using the National Sizing Survey, and we have contacted Dr. Tony Ruto for access to this algorithm. We are still awaiting his response.
    </p>
    <p><b>Key Libraries Used</b>
        <ul>
        <li>Microsoft.Kinect - used when interfacing with the Kinect Sensor</li>
          <li>System.Math - used for mathematical operations such as flooring and distance calculations</li>
          <li>System.Windows.Media - used for calculating colour values and getting RGB formats</li>
          <li>System.IO - used for reading/writing files / exporting the point cloud</li>
      </ul>
      </p>
    </card-content>

    <card-content>
    <h2>Prototype Iterations</h2>
    <p><b>Introduction</b><br>
      <p>
        We went through several iteration cycles when making our product, starting from an initial proof of concept console program, which simply established a connection to the Kinect 2.0 and generated a point cloud to a full measurements gathering solution.
      </p>
      <p><b>Iteration 1 - Initial Proof of concept</b><br>
      <p>
        Beginning with a small console application, we mapped out our intentions and familiarised ourselves with the Kinect SDK. At this stage, there was no formal UI or measurement extraction: the first thing we needed to do was capture a reasonable point cloud.
      </p>
      <img width="180px" src="img/Prototyping/iteration1.png">

      <p><b>Iteration 2 - QRCode integration, Early UI</b><br>
      <p>
        We divided our focus between these goals, iterating over a number of XAML-based user-interfaces- and each deriving from our early designs. We assembled  a point cloud using the Kinect SDK’s CameraSpacePoints, before operating on the resulting structure to ensure its suitability. This process involves applying a rotation matrix to each point, scaling the units and translating before outputting the cloud as finished. At the same time, we were also considering how we ought to display results to the user. We chose a QR code for its ease-of-use and wide recognisability.
      </p>
      <img width="310px" src="img/Prototyping/iteration2.png">

      <p><b>Iteration 3 - Live Camera feed, body pose detection</b><br>
      <p>
        So at this point, we had yet to properly track user movements and provide feedback for what they needed to do. Extending the UI, we added a live feed, displaying bodies recognised by the Kinect. In order to facilitate the scan, we needed users to adopt a particular pose, leading to the development of a body pose detection system that could identify whether or not users’ hands and feet were positioned correctly and how far from the scanner they were standing. Instructions are provided on-screen, explaining if users need to move closer or further away and how they need to stand. Once this is done, the system offers a short countdown before going ahead with the scan.
      </p>
      <img width="480px" src="img/Prototyping/iteration3.png">

      <p><b>Iteration 4 - Early ScanMeasure Integration </b><br>
      <p>
        With the point cloud processed and stored, we next began integrating Dr Tony Ruto’s ScanMeasure software that had previously seen success, but only with the Kinect 1. In conjunction with this software, the system was then able to deliver accurate body measurements, including fields such as height, chest and waist.  But our next task was to ensure that users find this information useful and readily available.
      </p>
      <img width="825px" src="img/Prototyping/iteration4.png">

      <p><b>Iteration 5 - Server and Companion App integration</b><br>
      <p>
       Once we had the windows desktop solution, we then started working on the companion android app and the local server for remote execution. On the local server, We chose to also implement a straightforward, and easy to extend RESTful API framework written in PHP. Some of the functionality of the server includes generating a unique ID number based on server time, which is later used to generate the QR code as well as remotely uploading the generated pointcloud vrml files using FTP. This also allowed the integration of ScanMeasure to be moved from the windows app to the Server to allow for remote execution. Similarly we also created a companion android app which could scan QR codes generated by the server and poll the server at frequent intervals in order to get back results, Display them to the User and allow for the saving of measurements; as well as an incognito mode which doesn't save any user measurements.
      </p>
      <img width="825px" src="img/Prototyping/iteration5.png">
  </card-content>

    <card-content>
    <h2>Point Cloud</h2>
      <p>An example of the resulting point cloud at half resolution and in colour:</p>
      <img width="300px" src="img/Prototyping/Mr.Red.jpg">
  </card-content>

    <!--<common-links select='5'></common-links>-->

  </core-header-panel>

</body>

</html>
